<!-- Mainly copied from ros_speech_recognition/launch/speech_recognition.launch -->
<!--
  For using speech recognition and text-to-speech.

  <include file="$(find riberry_startup)/launch/module_llm_speech_to_text.launch" >
    <arg name="filtered_audio_topic" value="input_audio" />
  </include>

  <include file="$(find riberry_startup)/launch/sound_play.launch" >
    <arg name="device" value="plughw:0,1" />
  </include>
-->

<launch>
  <arg name="audio_topic" default="audio" />
  <arg name="filtered_audio_topic" default="audio" />
  <arg name="audio_info_topic" default="audio_info" />

  <!-- audio capture from microphone -->
  <node name="audio_capture" pkg="audio_capture" type="audio_capture"
        respawn="true">
    <remap from="audio" to="$(arg audio_topic)" />
    <rosparam subst_value="true">
      format: wave
      channels: 1
      depth: 32
      sample_rate: 16000
      sample_format: S32LE
    </rosparam>
    <param name="device" value="plughw:0,0" />
  </node>

  <node name="webrtcvad_ros"
        pkg="riberry_startup" type="webrtcvad_ros.py"
        output="screen" >
    <rosparam>
      aggressiveness: 1
    </rosparam>
    <remap from="audio_data" to="$(arg filtered_audio_topic)" />
    <remap from="audio_info" to="$(arg audio_info_topic)" />
  </node>

  <node name="speech_to_text_google"
        pkg="riberry_startup" type="speech_to_text.py"
        respawn="true"
        output="screen">
    <remap from="audio" to="webrtcvad_ros/speech_audio"  />
    <remap from="audio_info" to="$(arg audio_info_topic)" />
    <remap from="speech_to_text" to="speech_to_text_raw"  />
  </node>

  <node name="speech_to_text_mode"
        pkg="riberry_startup" type="speech_to_text_mode.py"
        respawn="true" output="screen">
  </node>

</launch>
